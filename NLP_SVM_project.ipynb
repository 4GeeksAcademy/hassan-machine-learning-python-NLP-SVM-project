{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 url  is_spam\n",
      "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
      "1                             https://www.hvper.com/     True\n",
      "2                 https://briefingday.com/m/v4n3i4f3     True\n",
      "3   https://briefingday.com/n/20200618/m#commentform    False\n",
      "4                        https://briefingday.com/fan     True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "import tldextract\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_url(url):\n",
    "    # Parse the URL and extract components\n",
    "    parsed_url = urlparse(url)\n",
    "    path_tokens = re.split(r'\\W+', parsed_url.path)\n",
    "    return ' '.join([token for token in path_tokens if token])  # Join tokens back into a string\n",
    "\n",
    "# Apply preprocessing to each URL\n",
    "data['preprocessed'] = data['url'].apply(preprocess_url)\n",
    "\n",
    "# Vectorize the preprocessed URLs\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_features = vectorizer.fit_transform(data['preprocessed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract additional features from URLs\n",
    "def extract_features(url):\n",
    "    features = {}\n",
    "    parsed_url = urlparse(url)\n",
    "    domain_info = tldextract.extract(url)\n",
    "    \n",
    "    features['url_length'] = len(url)\n",
    "    features['path_length'] = len(parsed_url.path)\n",
    "    features['use_https'] = 1 if parsed_url.scheme == \"https\" else 0\n",
    "    features['num_subdomains'] = len(domain_info.subdomain.split('.')) if domain_info.subdomain else 0\n",
    "    keywords = ['login', 'verify', 'account', 'banking', 'secure', 'update']\n",
    "    features['keyword_usage'] = np.any([keyword in url for keyword in keywords])\n",
    "    \n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature extraction to URLs\n",
    "feature_columns = data['url'].apply(extract_features)\n",
    "\n",
    "# Convert extracted features to sparse matrix\n",
    "numeric_features = csr_matrix(feature_columns.astype(float).values)\n",
    "\n",
    "# Combine TF-IDF features with numeric features\n",
    "X_combined = hstack([tfidf_features, numeric_features])\n",
    "\n",
    "# Labels\n",
    "y = data['is_spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.88      0.91      0.90       455\n",
      "        True       0.69      0.60      0.64       145\n",
      "\n",
      "    accuracy                           0.84       600\n",
      "   macro avg       0.78      0.76      0.77       600\n",
      "weighted avg       0.83      0.84      0.83       600\n",
      "\n",
      "Confusion Matrix:\n",
      " [[416  39]\n",
      " [ 58  87]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the SVM\n",
    "model = SVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "Best parameters: {'C': 100, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.96      0.94       455\n",
      "        True       0.85      0.76      0.80       145\n",
      "\n",
      "    accuracy                           0.91       600\n",
      "   macro avg       0.89      0.86      0.87       600\n",
      "weighted avg       0.91      0.91      0.91       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Initialize the grid search\n",
    "grid_search = GridSearchCV(SVC(), param_grid, verbose=1, cv=3, n_jobs=-1)\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters found\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Evaluate the best model found by the grid search\n",
    "best_predictions = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, best_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.93      0.96      0.94       455\n",
      "        True       0.85      0.76      0.80       145\n",
      "\n",
      "    accuracy                           0.91       600\n",
      "   macro avg       0.89      0.86      0.87       600\n",
      "weighted avg       0.91      0.91      0.91       600\n",
      "\n",
      "Confusion Matrix:\n",
      " [[436  19]\n",
      " [ 35 110]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the SVM\n",
    "model = SVC(C=100, gamma='scale', kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, predictions))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.899954368391764\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation and print the mean accuracy\n",
    "scores = cross_val_score(SVC(C=10, gamma='scale', kernel='rbf'), X, y, cv=5)\n",
    "print(\"Mean cross-validation accuracy:\", scores.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to spam_detection_model.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "model_filename = 'spam_detection_model.pkl'\n",
    "\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "print(f'Model saved to {model_filename}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
